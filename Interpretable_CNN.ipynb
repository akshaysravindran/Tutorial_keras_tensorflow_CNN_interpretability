{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interpretable_CNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaysravindran/Tutorial_keras_tensorflow_CNN_interpretability/blob/main/Interpretable_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HfhkpLioqmu"
      },
      "source": [
        "\n",
        "# **Interpretable Convolutional Neural Network for EEG Analysis**\n",
        "\n",
        "## Install the required packages\n",
        "This jupyter notebook is intended to serve as a tutorial to guide you through different ways to explain the convolutional neural network model (CNN) applied for electroencephalography analysis. The tutorial will cover the following segments\n",
        "\n",
        "1) Training the model in keras<br>\n",
        "2) Creating your CNN model<br>\n",
        "3) Visualizing/ explainig the features learnt by the model using <br>\n",
        "\n",
        "\n",
        "*   Occlussion Sensitivity\n",
        "*   Activation Maximization\n",
        "*   Grad-Class Activation Map\n",
        "\n",
        "\n",
        "<br><br>\n",
        "The tutorial is based on the sample data provided throughout the workshop series. Here the EEG data is collected from a volunteer who performed about 10 repetitions of eyes open and eyes closed lasting about 10 seconds each. The trials were alternating in betweem. The data used here is already preprocessed and segmented into training and tuning set.\n",
        "<br><br>\n",
        "\n",
        "Please ensure *Dataset.mat* and *dataset.png* files sent as email (unzip first) is saved inside your local google drive under a folder called *uhbmi_workshop*<br><br>\n",
        "\n",
        "**Note:** <br>\n",
        "\n",
        "\n",
        "\n",
        "*   The code uses earlier versions of multiple packages (keras, tensorflow) due to compatability issues with keras-vis package. There is a recent package developed to work with the latest version of keras and tensorflow libraries by another group. However, these are still being developed and the activization maximisation had some glitches and needs exploration still. \n",
        "\n",
        "*   Here, gradCAM using the keras-vis package evaluates per image basis which is extremely slow.\n",
        "\n",
        "*   Some of the plotting features in google colab behaves differently, for e.g. plotting colorbar for MNE generated images is being showed as an extra subplot. Therefore, those lines are commented out in this version  as the issue is not diagnosed. These will work locally if you install the required packages in anaconda and run your code there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joQYlD5inNWP"
      },
      "source": [
        "\n",
        "%%capture\n",
        "if 1:\n",
        "  # Keras is running on top of tensorflow\n",
        "  !pip install tensorflow-gpu==1.15  # GPU\n",
        "  # Keras vis package for implementing the visualization methods\n",
        "  !pip install -U -I git+https://github.com/raghakot/keras-vis.git\n",
        "  # Library to plot EEG data \n",
        "  !pip install mne --upgrade\n",
        "  \n",
        "  # Downgrade to a lower version of keras\n",
        "  !pip install keras=='2.3.1'\n",
        "  !pip install matplotlib==2.2.3.\n",
        "  !pip install scipy==1.1.0\n",
        "  !pip install 'h5py<3.0.0'\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQVvnMOwEdn-"
      },
      "source": [
        "### Mount the google drive so that you can access the data saved there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px_em48nVbYv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnOOFavbpNwl"
      },
      "source": [
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-177JeinUMu"
      },
      "source": [
        "\n",
        "#%%\n",
        "#!pip uninstall matplotlib\n",
        "\n",
        "\n",
        "# General functions\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from scipy import signal\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "from sklearn.metrics import  f1_score, precision_score, recall_score\n",
        "import mne\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Keras functions\n",
        "from keras.models import Model\n",
        "from keras import regularizers, activations\n",
        "from keras.layers import Dense, Input, Flatten, Dropout\n",
        "from keras.layers.convolutional import Conv2D,  MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils import to_categorical   \n",
        "\n",
        "# Functions for visualization\n",
        "from vis.visualization import visualize_cam, visualize_activation\n",
        "from vis.utils import utils \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI9uDkDOpShc"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Make sure the required files are in the uhbmi_workshop folder in your google drive<br>\n",
        "\n",
        "\n",
        "1.   EEG should be in the format (number_of_examples x window_length x channels)\n",
        "<br>\n",
        "* Here we use EEG of 28 channels each window being 2.5 seconds long; Currently not using a test set as the purpose of this tutorial is just to show the implementation of the techniques and not reporting the accuracy and the example dataset is of small overall duration\n",
        "2.   Labels are in integers format <br>\n",
        "    *   0: Class 1 (eyes open) <br>\n",
        "    *   1: Class 2  (eyes closed)<br>\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_K4r091oW2Q"
      },
      "source": [
        "#%% Loading and preparing the data\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/uhbmi_workshop\")\n",
        "Loaded_data = sio.loadmat('Dataset.mat') #load data in matlab format   \n",
        " \n",
        "# Extract training and validation EEG    \n",
        "X_train     = Loaded_data['Training_set' ] \n",
        "X_valid     = Loaded_data['Tuning_set']  \n",
        "\n",
        "# Extract training and validation labels \n",
        "Y_train     = Loaded_data['Train_Y' ]  \n",
        "Y_train     = np.ravel(Y_train) \n",
        "\n",
        "Y_valid     = Loaded_data['Tuning_set_Y' ]\n",
        "Y_valid     = np.ravel(Y_valid)\n",
        "\n",
        "chanlocs     = Loaded_data['chanlocs' ] # EEG Channels\n",
        "print('The training and tuning set shapes are: ')\n",
        "print(np.shape(X_train), np.shape(X_valid)) # show the dimension of the data\n",
        "print(Y_train, Y_valid)\n",
        "\n",
        "print('Loaded the data')\n",
        "\n",
        "# Display the original data structure and how data was partitioned\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.imshow(mpimg.imread('dataset.png'))\n",
        "plt.box(False)\n",
        "frame1       = plt.gca()\n",
        "frame1.axes.yaxis.set_ticklabels([])\n",
        "frame1.axes.yaxis.set_ticks([])\n",
        "frame1.axes.xaxis.set_ticklabels([])\n",
        "frame1.axes.xaxis.set_ticks([])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B1nRMkeSD2K"
      },
      "source": [
        "## Data visualizing\n",
        "\n",
        "Understand the data being classified prior to developing the model <br>\n",
        "* Visualize one example data from each  class in raw data format\n",
        "* Plot the mean PSD and topoplot in alpha band"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoawSCECSCWU"
      },
      "source": [
        "#%% Plot two sample inputs from either classes\n",
        " \n",
        "ch,example_ID                         = 28,10\n",
        "example_c0                 = X_train[example_ID].copy()\n",
        "example_c1                 = X_train[-example_ID].copy()\n",
        "\n",
        "offset                     = np.max(abs(example_c0))\n",
        "for i in range(ch):\n",
        "    example_c0[:,i]        =  example_c0[:,i]-(i*offset)  \n",
        "    example_c1[:,i]        =  example_c1[:,i]-(i*offset)  \n",
        "    \n",
        "    \n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "ax1                       = plt.subplot(121)\n",
        "plt.plot(example_c0,color = 'k',linewidth = 1)\n",
        "plt.box(False)\n",
        "frame1                    = plt.gca()\n",
        "frame1.axes.yaxis.set_ticklabels([])\n",
        "frame1.axes.yaxis.set_ticks([])\n",
        "frame1.axes.xaxis.set_ticklabels([])\n",
        "frame1.axes.xaxis.set_ticks([])\n",
        "plt.title('Eyes Open Example')\n",
        "\n",
        "ax1                       = plt.subplot(122)\n",
        "plt.plot(example_c1,color = 'k',linewidth = 1)\n",
        "plt.box(False)\n",
        "frame1                    = plt.gca()\n",
        "frame1.axes.yaxis.set_ticklabels([])\n",
        "frame1.axes.yaxis.set_ticks([])\n",
        "frame1.axes.xaxis.set_ticklabels([])\n",
        "frame1.axes.xaxis.set_ticks([])\n",
        "plt.title('Eyes Closed Example')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN9FGoBBX1ZK"
      },
      "source": [
        "\n",
        "1.   Plot the mean power spectral density in the parietal and occipital channels for the same set of examples\n",
        "\n",
        "2.   Plot the alpha power difference between examples using a topoplot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqaA00IEYXIa"
      },
      "source": [
        "# Plot the PSD and the topoplot for alpha band power\n",
        "NFFT           = 256\n",
        "psd_all        = np.zeros((len(X_valid), NFFT//2+1,ch) ) \n",
        "\n",
        "# Compute PSD\n",
        "for i in range(len(X_valid)):\n",
        "    for chan in range(ch):\n",
        "        freqs, psd_all[i,:,chan]    = signal.welch(X_valid[i,:,chan], fs=100., window='hann', nfft=NFFT)\n",
        "\n",
        "class0_loc   = np.where(Y_valid  ==0)[0]\n",
        "class1_loc   = np.where(Y_valid  ==1)[0]\n",
        "\n",
        "fig          = plt.figure(figsize=(10, 5))\n",
        "ax1          = plt.subplot(1,2,1)\n",
        "plt.plot(freqs, 10*np.log10(np.mean(np.mean(psd_all[class0_loc,:,-8:],axis=2),axis=0)))         \n",
        "plt.plot(freqs, 10*np.log10(np.mean(np.mean(psd_all[class1_loc,:,-8:],axis=2),axis=0)))\n",
        "plt.xlim([0,45])\n",
        "plt.ylim([-20,20])\n",
        "plt.title('Parietal & occipital channels (mean PSD) ', fontsize = 14)\n",
        "plt.xlabel('Frequency', fontsize = 14)\n",
        "plt.ylabel('Power (dB)', fontsize = 14)\n",
        "plt.legend(['Eyes open class','Eyes closed class'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract alpha power\n",
        "\n",
        "alpha_freq   = np.where(np.logical_and(freqs>=8 , freqs < 13))[0]\n",
        "pow_c0_alpha = np.mean(np.mean(psd_all[class0_loc][:,alpha_freq,:],axis=1),axis=0)\n",
        "pow_c1_alpha = np.mean(np.mean(psd_all[class1_loc][:,alpha_freq,:],axis=1),axis=0)\n",
        "\n",
        "\n",
        "ax2          = plt.subplot(1,2,2)\n",
        "plt.title('Eyes Closed - Eyes open (Alpha power)', fontsize = 14)\n",
        "im, cn       = mne.viz.plot_topomap(pow_c1_alpha - pow_c0_alpha, chanlocs,sphere = np.max(chanlocs)-0.02 )\n",
        "cbar         = fig.colorbar(im,orientation=\"horizontal\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPfM5S5qbOV"
      },
      "source": [
        "## Prepare the data for training\n",
        "The following cell is used to prepare and later shuffle both the training and validation set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u30oVEgXoap2"
      },
      "source": [
        "\n",
        "# Normalize data\n",
        "a           = np.max(np.abs(X_train))   \n",
        "X_train     = X_train/a # Normalize to -1, 1 range\n",
        "X_valid     = X_valid/a # Normalize to -1, 1 range\n",
        "\n",
        "# Reshape to get the shape (num_cases x time_samples x channels x 1); Here 1 in the end is the number of channels (3 in RGB image)\n",
        "# The CNN in keras requires data to be converted into the following dimension \n",
        "X_train     = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "X_valid     = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], 1))\n",
        "\n",
        "\n",
        "#%% Shuffle the data\n",
        "\n",
        "# Shuffle the validation data \n",
        "order       = np.arange(len(Y_valid))\n",
        "np.random.shuffle(order)\n",
        "X_valid     = X_valid[order]\n",
        "Y_valid     = Y_valid[order]\n",
        "\n",
        "\n",
        "# Shuffle the training data \n",
        "order       = np.arange(len(Y_train))\n",
        "np.random.shuffle(order)\n",
        "X_train     = X_train[order]\n",
        "Y_train     = Y_train[order]\n",
        "\n",
        "\n",
        "print('Shuffled both the training and the test data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJxMWPkqdFV"
      },
      "source": [
        "## The CNN model\n",
        "\n",
        "Create a CNN model to classify the EEG windows into either eyes closed or eyes open class. The model is implemented in Keras using tensorflow backend. <br>\n",
        "\n",
        "## Initialization\n",
        "Initialize the hyper parameters of the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__CSQEyfocjW"
      },
      "source": [
        "#%% Create the CNN model\n",
        "\n",
        "# Initialization\n",
        "ch                        = 28  #number of channels \n",
        "time_samples              = 250 #samples per example-> sampling rate is 100 Hz\n",
        "input_shape               = (time_samples, ch,1)  # input dimension\n",
        "num_row , num_col         = 10, 1 # Filter dimensions for the convolution layers\n",
        "pool_dim                  = (3, 1) # Max Pooling dimension\n",
        "strides_conv,strides_pool = (1 , 1), (2 , 1) # Stride dimension for conv and pool layers\n",
        "padding                   = 'valid' # Type of zero padding (currently set to not pad)\n",
        "num_filters               = 8 # number of convolutional filters per layer\n",
        "batch_size                = 64 # mini-batch size for traning the model\n",
        "epoch_len                 = 1000 # number of epochs to train the model for\n",
        "num_classes               = len(np.unique(Y_valid)) # number of classes to decode\n",
        "BW_Tall                   = 'Weights.hdf5' # file to save the model weights in  \n",
        "reg                       = regularizers.l2(0.01) # kernal regularizer\n",
        "act                       = 'elu'\n",
        "# Callbacks for early stopping condition and saving best weights\n",
        "callback_array            = [EarlyStopping(monitor='val_loss', min_delta = 0, patience = 3, verbose = 0, mode = 'auto'), # stop training if the validation loss do not improve in 3 successive epochs\n",
        "                            ModelCheckpoint(BW_Tall, monitor='val_loss', verbose = 1, save_best_only = True, mode = 'auto') ] # save the weights only if the validation loss improved\n",
        "                            \n",
        "print('Hyper parameters and variables initialized')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZDHQv0Nq17u"
      },
      "source": [
        "## CNN Architecture\n",
        "\n",
        "* 1 Input layer<br>\n",
        "* 7 Convolution layers<br>\n",
        "* 3 Max pooling layers<br>\n",
        "* 1 Dropout layer<br>\n",
        "* 1 Flatten layer<br>\n",
        "* 1 Output fully connected layer<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE7fihREoea2"
      },
      "source": [
        "#%% Model Architecure\n",
        "Allinput_img              = Input(shape=(input_shape)) # input tensor\n",
        "\n",
        "# Block 1\n",
        "x = Conv2D(num_filters, (num_row, num_col), strides=strides_conv, padding=padding, activation = act, name='Temporal_conv_B1_1', kernel_regularizer = reg)(Allinput_img)\n",
        "x = Conv2D(num_filters, (num_row, num_col), strides=strides_conv, padding=padding, activation = act, name='Temporal_conv_B1_2', kernel_regularizer = reg)(x)   \n",
        "x = MaxPooling2D(pool_dim, strides = strides_pool, name='Temporal_pool_L1')(x) \n",
        "\n",
        "# Block 2\n",
        "x = Conv2D(num_filters, (num_row, num_col), strides=strides_conv, padding=padding, activation = act, name='Temporal_conv_B2_1', kernel_regularizer = reg)(x)\n",
        "x = Conv2D(num_filters, (num_row, num_col), strides=strides_conv, padding=padding, activation = act, name='Temporal_conv_B2_2', kernel_regularizer = reg)(x)   \n",
        "x = MaxPooling2D(pool_dim, strides = strides_pool, name='Temporal_pool_L2')(x) \n",
        "\n",
        "# Block 3\n",
        "x = Conv2D(num_filters, (num_row, num_col), strides=strides_conv, padding=padding, activation = act, name='Temporal_conv_B3_1', kernel_regularizer = reg)(x)\n",
        "x = Conv2D(num_filters, (num_row, num_col), strides=strides_conv, padding=padding, activation = act, name='Temporal_conv_B3_2', kernel_regularizer = reg)(x)   \n",
        "x = MaxPooling2D(pool_dim, strides = strides_pool, name='Temporal_pool_L3')(x) \n",
        "\n",
        "\n",
        "x = Conv2D(num_filters, (1, ch), strides=strides_conv, padding=padding, activation = act, name='Spatial_conv', kernel_regularizer = reg)(x) # Do channel wise convolution\n",
        "x = Flatten()(x)   \n",
        "# x = (Dense(32, activation = 'elu',name = 'dense'))(x) # If you wish to add a fully connected layer in addition.\n",
        "x = Dropout(0.4, name = 'Alldrop1')(x)\n",
        "Out = (Dense(num_classes, activation = 'softmax',name = 'Allout'))(x)\n",
        "\n",
        "model=Model(Allinput_img, Out)   \n",
        "model.compile(optimizer=Adam(lr=0.000005), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "model.summary() # See the model architecture      \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGgL-aSjq5Bs"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4yusNgdohan"
      },
      "source": [
        "#%% Train the model\n",
        "\n",
        "hist        = model.fit(X_train, to_categorical(Y_train), # validation data\n",
        "                    batch_size      = batch_size, # mini batch size\n",
        "                    epochs          = epoch_len, # number of epochs to train the model\n",
        "                    validation_data = (X_valid, to_categorical(Y_valid)), # Validation data\n",
        "                    shuffle         = True, # Shuffle the data after every epoch\n",
        "                    callbacks       = callback_array, # the type of callbacks needed\n",
        "                    verbose         = 1) # The details of training log to display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS6gnAwIq91M"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XXUPb-Ooi-X"
      },
      "source": [
        "#%% Evaluate the model\n",
        "#model.load_weights(BW_Tall)  # Load the model weights with the best validation loss\n",
        "\n",
        "val_ACC     = model.evaluate(X_valid, to_categorical(Y_valid))\n",
        "train_ACC   = model.evaluate(X_train, to_categorical(Y_train))\n",
        "\n",
        "# Evaluate different performance metrics\n",
        "y_pred      = np.argmax(model.predict(X_valid),axis=1) \n",
        "F_score     = f1_score(Y_valid, y_pred, average=\"weighted\")\n",
        "P_score     = precision_score(Y_valid, y_pred, average=\"weighted\")\n",
        "R_score     = recall_score(Y_valid, y_pred, average=\"weighted\")\n",
        "\n",
        "print((val_ACC, F_score, P_score, R_score))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Training and Validation Loss with iteration')\n",
        "plt.ylabel('Categorical Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw-YPo-Zq_1N"
      },
      "source": [
        "## Visualizing the CNN model\n",
        "\n",
        "Implement the 3 different methods of explaining the CNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El7RbQAc0Wck"
      },
      "source": [
        "### Method 1: Occlusion Method\n",
        "Find the change in performance for the model by occluding each channels. Identify the relevance for each of the channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G042y3n-hzxs"
      },
      "source": [
        "occlusion_acc_change               = np.zeros((ch,1))\n",
        "Location                           = np.where(Y_valid == 1)[0] # Eyes closed data  \n",
        "validation_acc                     = model.evaluate(X_valid[Location].copy(), to_categorical(Y_valid[Location]))[1]                                                       \n",
        "for i in range(ch): \n",
        "    temp_data                      = X_valid[Location].copy()\n",
        "    temp_data[:,:,i,:]             = 0.\n",
        "    occlusion_acc_change[i]                =  model.evaluate(temp_data, to_categorical(Y_valid[Location]))[1] - validation_acc    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndjbRlAbp8g7"
      },
      "source": [
        "Visualize the output of the occlusion method. The blue section corresponds to the channel which has the most relevance for correctly predicting the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho8ws9NlqIfx"
      },
      "source": [
        "# Generate the topoplot to indicate the channel occlusion sensitivity for the model\n",
        "\n",
        " \n",
        "\n",
        "fig          = plt.figure(figsize=(6, 5))\n",
        "ax           = fig.add_subplot(111)\n",
        "im, cn       = mne.viz.plot_topomap(occlusion_acc_change[:,0]*100, chanlocs,sphere = np.max(chanlocs)-0.02 )\n",
        "cbar         = plt.colorbar(im)\n",
        "plt.title('Occlussion Channel Sensitivity', fontsize = 14, fontweight = 'bold')\n",
        "cbar.set_label('% change in performance', rotation = 90, fontsize = 14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2mpGcESaXYS"
      },
      "source": [
        "### Method 2: Activation Maximization\n",
        " Generates the model input that maximizes the output of all filter_indices in the given layer_idx ( output layer units corresponds to each class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYc3zWWFaUDw"
      },
      "source": [
        "# Layers to visualize\n",
        "layer_idx                          = utils.find_layer_idx(model, 'Allout') # identify the layer by the name\n",
        "model.layers[layer_idx].activation = activations.linear # Change the softmax activation function with linear\n",
        "model                              = utils.apply_modifications(model)\n",
        "\n",
        "Maximized_input                    = []\n",
        "for iteration in range(5): # Repeat 10 times\n",
        "    for output_idx in np.arange(2):  \n",
        "            max_input              = visualize_activation(model, # The trained model\n",
        "                                                            -1,\n",
        "                                                            tv_weight      = 1.,  # The weight param for TotalVariation regularization loss\n",
        "                                                            lp_norm_weight = 10.,  # The weight param for LPNorm regularization loss\n",
        "                                                            act_max_weight = 10., # The weight param for ActivationMaximization loss\n",
        "                                                            filter_indices = output_idx, # filter indices within the layer to be maximized\n",
        "                                                            input_range    = (-1.,1.),   # This is used to rescale the final optimized input to the given range\n",
        "                                                            max_iter       = 500,        # number of iterations to run\n",
        "                                                            verbose        = True)       # whether to print details during the iterations\n",
        "            Maximized_input.append(max_input)\n",
        "\n",
        "print('Activation Maximization computation done')     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW6ntpc0qqLS"
      },
      "source": [
        "Generate the outputs of the activation maximization method. <br>\n",
        "\n",
        "Occlusion sensitivity is model independent approach where it depends on the input being fed into the models and how the model prioritizes each of the channels. AM on the otherhand, is a model specific method wherein we generate the ideal input which maximizes the prediction probability to belong to a particular class of interest. \n",
        "\n",
        "First we evaluate whether the model was indeed able to learn the topoplot distribution using this approach. <br>\n",
        "\n",
        "Then we check whether the model understood that alpha power was indeed the most relevant feature to discriminate the classes. We do that by computing the power spectral density of the generate inputs and compare across the classes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHGQ0I26rl0T"
      },
      "source": [
        "# Compute PSD from the maximized input generated above from each of the classes\n",
        "# Plot the PSD and the topoplot for alpha band power\n",
        "NFFT           = 256\n",
        "PSD          = np.zeros((len(Maximized_input), NFFT//2+1,ch) )  \n",
        "for i in range(len(Maximized_input)):\n",
        "    for chan in range(ch):\n",
        "        freqs, psd    = signal.welch(Maximized_input[i][:,chan,0], fs=100., window='hann', nfft=NFFT)\n",
        "        PSD[i,:,chan] = psd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract alpha power\n",
        "alpha_freq   = np.where(np.logical_and(freqs>=8 , freqs < 13))[0]\n",
        "pow_c0_alpha =     np.mean(np.mean(PSD[::2,alpha_freq,:], axis=1), axis=0) # every other element belong to the same class\n",
        "pow_c1_alpha =     np.mean(np.mean(PSD[1::2,alpha_freq,:], axis=1), axis=0)\n",
        "\n",
        "# Generate the topoplot distribution of power difference in alpha (inputs for each class generated by AM method)\n",
        " \n",
        "fig          = plt.figure(figsize=(6, 5))\n",
        "im, cn       = mne.viz.plot_topomap(pow_c1_alpha - pow_c0_alpha, chanlocs,sphere = np.max(chanlocs)-0.02 )\n",
        "plt.title('Eyes Open - Eyes Closed (Act Max)', fontsize = 14, fontweight = 'bold')\n",
        "cbar         = plt.colorbar(im)\n",
        "\n",
        "\n",
        "\n",
        "# Compute the psd in the parietal occipital region\n",
        "PSD_c0_mean  =     np.mean(np.mean(PSD[::2,:,-8:], axis=0), axis=1) # Last 8 channels belong to parietal and occipital channels\n",
        "PSD_c1_mean  =     np.mean(np.mean(PSD[1::2,:,-8:], axis=0), axis=1)\n",
        "\n",
        "\n",
        "# Generate the mean PSD for the parietal occipital channels\n",
        "fig          = plt.figure(figsize=(6, 5))\n",
        "plt.plot(freqs, (PSD_c0_mean))\n",
        "plt.plot(freqs, (PSD_c1_mean))\n",
        "plt.xlim([0,45])\n",
        "plt.title('Parietal & occipital channels (mean power spectral density) ', fontsize = 14)\n",
        "plt.xlabel('Frequency', fontsize = 14)\n",
        "plt.ylabel('Power', fontsize = 14)\n",
        "plt.legend(['Eyes open class','Eyes closed class'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6qwAqkBalrQ"
      },
      "source": [
        "### Method 3: Grad-CAM\n",
        "Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of filter_indices in layer_idx. We will take the same sample image and get the gradCAM corresponding to the last convolution layer to get the heatmap. \n",
        "\n",
        "For the sake of the tutorial, we are performing this on 12 examples randomly sampled from the correctly identified data points\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYoKIi7Earpf"
      },
      "source": [
        "# Select correctly identified examples of eyes closed\n",
        "Location                           = np.where(np.logical_and(y_pred == Y_valid,  Y_valid == 1))[0]       \n",
        "order                              = np.arange(len(Location))\n",
        "np.random.shuffle(order)\n",
        "Location                           = Location[order]\n",
        "\n",
        "\n",
        "Location_0                         = np.where(np.logical_and(y_pred == Y_valid,  Y_valid == 0))[0]       \n",
        "order                              = np.arange(len(Location_0))\n",
        "np.random.shuffle(order)\n",
        "Location_0                         = Location_0[order]\n",
        "\n",
        "         \n",
        "# Layers to estimate gradCAM based on            \n",
        "layer_idx1                         = utils.find_layer_idx(model, 'Temporal_pool_L3') # # identify the layer by the name \n",
        "\n",
        "# Initialize the variables\n",
        "p, num_examples                    = 0, 12  # num_examples : total number of examples to obtain CAM for \n",
        "gradCAMs                           = np.zeros((num_examples*2, time_samples, ch))     \n",
        "\n",
        "for iteration in range(num_examples):     \n",
        "    for output_idx in np.arange(2): \n",
        "        if output_idx == 0:\n",
        "            seed                    = X_valid[Location_0[iteration]] # \n",
        "        else:\n",
        "            seed                    = X_valid[Location[iteration]] # \n",
        "            \n",
        "        gradCAMs[p]                 = visualize_cam(model,  # The trained model\n",
        "                                                   -1, # The layer index whose filters needs to be visualized.\n",
        "                                                   filter_indices        = output_idx, # filter indices within the layer to be maximized. \n",
        "                                                   penultimate_layer_idx = layer_idx1, # The pre-layer to layer_idx whose feature maps should be used to compute gradients wrt filter output. \n",
        "                                                   seed_input            = seed) #  The input image for which activation map needs to be visualized.\n",
        "        p=p+1        \n",
        "\n",
        "print('GradCAM computation done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BecJYseTKwIp"
      },
      "source": [
        "Check whether using gradCAM method, we can identify the most relevant channels. Plot the topoplot based on the mean gradCAM across each channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfNrrq0NKsvu"
      },
      "source": [
        "rcParams['figure.figsize'] = 6, 4\n",
        "fig          = plt.figure()\n",
        "gradcam_c1   = np.mean(np.mean(gradCAMs[1::2], axis = 0), axis = 0)\n",
        "ax           = fig.add_subplot(111)\n",
        "im, cn       = mne.viz.plot_topomap(gradcam_c1, chanlocs,sphere = np.max(chanlocs)-0.02 )\n",
        "plt.title('Eyes Closed Class GradCAM', fontsize = 14, fontweight = 'bold')\n",
        "#cbar         = plt.colorbar(im)\n",
        "#cbar.set_label('Importance score', rotation = 90, fontsize = 14)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aB-m8hZhdem"
      },
      "source": [
        "#%%  Save all the outputs for exploring more locally \n",
        "sio.savemat('/content/drive/My Drive/uhbmi_workshop/Visualized_outputs.mat', {'Maximized_input':Maximized_input,'gradCAMs':gradCAMs, 'occlusion_acc_change':occlusion_acc_change })\n",
        "\n",
        "# Save the scores and training history\n",
        "sio.savemat('/content/drive/My Drive/uhbmi_workshop/Score.mat', {'val_loss':hist.history['val_loss'],\n",
        "                                'val_acc':hist.history['val_accuracy'],'loss':hist.history['loss'],\n",
        "                                'acc':hist.history['accuracy'],  'val_ACC':val_ACC,\n",
        "                                'train_ACC':train_ACC,\n",
        "                                'F_score':F_score ,'P_score':P_score, 'R_score':R_score })  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eI6i5n_LDV7"
      },
      "source": [
        "##Extra: \n",
        "Check how gradCAM works when you have a long window with both classes in it. Here we feed an input of 5 second long window with the first 2.5 seconds belonging to eyes closed condition and the last 2.5 seconds belonging to eyes open condition. We then compute the gradCAM for this input and ask the model which part in this data the model believes to most contribute to eyes closed condition. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoMQeRBjLfRk"
      },
      "source": [
        "\n",
        "chanlocs_name =  ['FP1',\t'FP2',\t'F7',\t'F3',\t'Fz',\t\n",
        "                'F4',\t'F8',\t'FC5',\t'FC1',\t'FC2',\t\n",
        "                'FC6',\t'T7',\t'C3',\t'Cz',\t'C4',\t\n",
        "                'T8',\t'CP5',\t'CP1',\t'CP2',\t'CP6',\t\n",
        "                'P7',\t'P3',\t'Pz',\t'P4',\t'P8',\t\n",
        "                'O1',\t'Oz',\t'O2']\n",
        "\n",
        "test         = Loaded_data['test' ].T \n",
        "test_copy    = test.copy()\n",
        "test         = np.reshape(test, (1, test.shape[0], test.shape[1], 1))/a # Normalize with the same constant as before\n",
        "\n",
        "\n",
        "# Adding offset for plotting purpose\n",
        "offset       = np.max(abs(test_copy))\n",
        "for i in range(ch):\n",
        "    test_copy[:,i] =  test_copy[:,i]-(i*offset)  \n",
        "     \n",
        "# Generate gradCAM for continous data including both classes\n",
        "\n",
        "cont_data    = visualize_cam(model,  # The trained model\n",
        "                            -1, # The layer index whose filters needs to be visualized.\n",
        "                            filter_indices        = 1, # filter indices within the layer to be maximized. \n",
        "                            penultimate_layer_idx = layer_idx1, # The pre-layer to layer_idx whose feature maps should be used to compute gradients wrt filter output. \n",
        "                            seed_input            = test) #  The input image for which activation map needs to be visualized.\n",
        "\n",
        "# Plot the EEG time series with offset\n",
        "plt.figure(figsize=(20, 15))\n",
        "ax1          = plt.subplot(211)\n",
        "plt.plot([time_samples/2, time_samples/2],[np.min(test_copy)-1, np.max(test_copy)+1],color='r',linewidth=2,linestyle='dashed')\n",
        "plt.plot(test_copy,color = 'k',linewidth = 1)\n",
        "plt.text(int(time_samples*0.15), np.max(test_copy) + 0.1,'Eyes Closed', fontsize = 14)\n",
        "plt.text(int(time_samples*0.65), np.max(test_copy) + 0.1,'Eyes Open', fontsize = 14)\n",
        "\n",
        "plt.box(False)\n",
        "frame1 = plt.gca()\n",
        "frame1.axes.yaxis.set_ticklabels([])\n",
        "frame1.axes.yaxis.set_ticks([])\n",
        "frame1.axes.xaxis.set_ticklabels([])\n",
        "\n",
        "# Show the gradcam heatmap\n",
        "ax2          = plt.subplot(212 ,sharex = ax1)\n",
        "im           = plt.imshow(cont_data.T, aspect='auto')\n",
        "plt.yticks(np.arange(0,28,1))\n",
        "plt.xlabel('Time (s)')\n",
        "frame2       = plt.gca()\n",
        "frame2.axes.yaxis.set_ticklabels(chanlocs_name)\n",
        "cbar         = plt.colorbar(im, orientation=\"horizontal\")\n",
        "cbar.set_label('attention score', fontsize = 14)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}